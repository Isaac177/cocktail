{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQlqsX15vPhJ73AD1JD/qN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaac177/cocktail/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive\n",
        "!mkdir /content/drive"
      ],
      "metadata": {
        "id": "O0_sDKw1lGe9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_v0LqTslYRR",
        "outputId": "d1a5b6fb-9ffb-4f13-d305-c4198d01df7d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJX0zULY-rBx",
        "outputId": "ec513ead-9237-4c3f-bf8f-099dfbc2c637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in Google Drive root:\")\n",
        "print(os.listdir('/content/drive/MyDrive/Colab Notebooks'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6cdzFY1_Fxk",
        "outputId": "9f0d21ad-9d4a-4db7-c6e2-9dd74190b030"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in Google Drive root:\n",
            "['Copy of COLAB - CIFAR10_image_classification.ipynb', 'Neural-1.ipynb', 'Assignement 3.ipynb', 'Assignment_2.ipynb', 'Copy of cifar10_tutorial.ipynb', 'archive.zip', 'Untitled0.ipynb', 'Untitled2.ipynb', 'extracted_archive', 'Untitled1.ipynb', 'final.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Updated path to your zip file\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/archive.zip'\n",
        "\n",
        "# Create a directory to extract to\n",
        "extract_path = '/content/drive/MyDrive/Colab Notebooks/extracted_archive'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "!mkdir -p \"{extract_path}\"\n",
        "\n",
        "# Extract the contents\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Let's see what was extracted\n",
        "print(\"\\nExtracted contents:\")\n",
        "print(os.listdir(extract_path))"
      ],
      "metadata": {
        "id": "Ob3P_DV-_aNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Let's look inside the dogcat folder\n",
        "dogcat_path = '/content/drive/MyDrive/Colab Notebooks/extracted_archive/dogcat'\n",
        "print(\"Contents of dogcat folder:\")\n",
        "print(os.listdir(dogcat_path))\n",
        "\n",
        "# Let's also count the number of files and check a few sample paths\n",
        "sample_files = os.listdir(dogcat_path)[:5]  # Show first 5 files\n",
        "print(\"\\nFirst 5 files:\")\n",
        "for file in sample_files:\n",
        "    print(file)\n",
        "\n",
        "# Count total files\n",
        "total_files = len(os.listdir(dogcat_path))\n",
        "print(f\"\\nTotal number of files: {total_files}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHDAjnuaB1bQ",
        "outputId": "16b642cc-ae65-4447-d32d-dc7b705f154c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of dogcat folder:\n",
            "['test1', 'train', 'validation', 'sampleSubmission.csv']\n",
            "\n",
            "First 5 files:\n",
            "test1\n",
            "train\n",
            "validation\n",
            "sampleSubmission.csv\n",
            "\n",
            "Total number of files: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check contents of each directory\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/extracted_archive/dogcat'\n",
        "\n",
        "# Check train folder\n",
        "train_path = os.path.join(base_path, 'train')\n",
        "print(\"Number of files in train folder:\", len(os.listdir(train_path)))\n",
        "print(\"Sample train files:\", os.listdir(train_path)[:5])\n",
        "\n",
        "# Check validation folder\n",
        "val_path = os.path.join(base_path, 'validation')\n",
        "print(\"\\nNumber of files in validation folder:\", len(os.listdir(val_path)))\n",
        "print(\"Sample validation files:\", os.listdir(val_path)[:5])\n",
        "\n",
        "# Check test folder\n",
        "test_path = os.path.join(base_path, 'test1')\n",
        "print(\"\\nNumber of files in test folder:\", len(os.listdir(test_path)))\n",
        "print(\"Sample test files:\", os.listdir(test_path)[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLCB-Z4cB-x5",
        "outputId": "76cf46ca-b01a-4e6e-86db-cf1d719d05fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in train folder: 2\n",
            "Sample train files: ['cats', 'dogs']\n",
            "\n",
            "Number of files in validation folder: 2\n",
            "Sample validation files: ['cats', 'dogs']\n",
            "\n",
            "Number of files in test folder: 1\n",
            "Sample test files: ['test1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check the number of images in each folder\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/extracted_archive/dogcat'\n",
        "\n",
        "# Count images in train folders\n",
        "train_cats = len(os.listdir(os.path.join(base_path, 'train/cats')))\n",
        "train_dogs = len(os.listdir(os.path.join(base_path, 'train/dogs')))\n",
        "\n",
        "# Count images in validation folders\n",
        "val_cats = len(os.listdir(os.path.join(base_path, 'validation/cats')))\n",
        "val_dogs = len(os.listdir(os.path.join(base_path, 'validation/dogs')))\n",
        "\n",
        "# Count test images\n",
        "test_images = len(os.listdir(os.path.join(base_path, 'test1/test1')))\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(f\"Cats: {train_cats}, Dogs: {train_dogs}\")\n",
        "print(\"\\nValidation Data:\")\n",
        "print(f\"Cats: {val_cats}, Dogs: {val_dogs}\")\n",
        "print(\"\\nTest Data:\")\n",
        "print(f\"Total test images: {test_images}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OTHsIz6j4Y2",
        "outputId": "710aef8c-7df2-45ac-d562-993c244af7be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "Cats: 12500, Dogs: 12500\n",
            "\n",
            "Validation Data:\n",
            "Cats: 4000, Dogs: 4000\n",
            "\n",
            "Test Data:\n",
            "Total test images: 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define basic constants\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/extracted_archive/dogcat'\n",
        "\n",
        "# Verify paths and print dataset information\n",
        "print(\"Checking dataset structure...\")\n",
        "print(\"\\nTraining Data:\")\n",
        "print(f\"Cats: {len(os.listdir(os.path.join(BASE_PATH, 'train/cats')))}\")\n",
        "print(f\"Dogs: {len(os.listdir(os.path.join(BASE_PATH, 'train/dogs')))}\")\n",
        "print(\"\\nValidation Data:\")\n",
        "print(f\"Cats: {len(os.listdir(os.path.join(BASE_PATH, 'validation/cats')))}\")\n",
        "print(f\"Dogs: {len(os.listdir(os.path.join(BASE_PATH, 'validation/dogs')))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjqmAcgxnfwI",
        "outputId": "3532b34a-5bb6-4c27-b8ce-d1083685bdc0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking dataset structure...\n",
            "\n",
            "Training Data:\n",
            "Cats: 12500\n",
            "Dogs: 12500\n",
            "\n",
            "Validation Data:\n",
            "Cats: 4000\n",
            "Dogs: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data generators with augmentation for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values\n",
        "    rotation_range=20,  # Randomly rotate images by up to 20 degrees\n",
        "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically\n",
        "    shear_range=0.2,  # Shear transformations\n",
        "    zoom_range=0.2,  # Random zoom\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    fill_mode='nearest'  # Strategy for filling in newly created pixels\n",
        ")\n",
        "\n",
        "# Only rescaling for validation data (no augmentation needed)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create generators for training and validation\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(BASE_PATH, 'train'),\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    os.path.join(BASE_PATH, 'validation'),\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Let's look at the generator settings\n",
        "print(\"\\nTraining Generator:\")\n",
        "print(f\"Found {train_generator.samples} images belonging to {len(train_generator.class_indices)} classes\")\n",
        "print(\"\\nValidation Generator:\")\n",
        "print(f\"Found {validation_generator.samples} images belonging to {len(validation_generator.class_indices)} classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQqyfLYNn6ie",
        "outputId": "efa3ef9b-cc61-4803-e14a-de13b8b5f9b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 images belonging to 2 classes.\n",
            "Found 8000 images belonging to 2 classes.\n",
            "\n",
            "Training Generator:\n",
            "Found 25000 images belonging to 2 classes\n",
            "\n",
            "Validation Generator:\n",
            "Found 8000 images belonging to 2 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained VGG16 model\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',  # Use pre-trained weights\n",
        "    include_top=False,   # Don't include the final classification layers\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)  # Specify input image size\n",
        ")\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create our model\n",
        "model = models.Sequential([\n",
        "    # Base VGG16 model\n",
        "    base_model,\n",
        "\n",
        "    # Add our classification layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification (dog or cat)\n",
        "])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "1hP3W7GpoFD7",
        "outputId": "0897f21f-0f66-48bc-9652-0fa422260be3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │      \u001b[38;5;34m12,845,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m513\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,560,769\u001b[0m (105.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,560,769</span> (105.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,846,081\u001b[0m (49.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,846,081</span> (49.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Model Compilation and Training\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add early stopping to prevent overfitting\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=2,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIFdWLVMoVEV",
        "outputId": "3620dfe9-b5e1-460e-cacf-f204a3d173d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m163/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23:37\u001b[0m 14s/step - accuracy: 0.7405 - loss: 0.7099"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot training history\n",
        "def plot_training_results(history):\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'], label='Training Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the results (run this after training completes)\n",
        "plot_training_results(history)\n",
        "\n",
        "# Print final metrics\n",
        "final_accuracy = history.history['val_accuracy'][-1]\n",
        "final_loss = history.history['val_loss'][-1]\n",
        "print(f\"\\nFinal Validation Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "9bD0UkIvLJJg",
        "outputId": "9cd3b3cc-bede-4d84-8b2e-79fd5dc5d99a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9f4adb5afb50>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Plot the results (run this after training completes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mplot_training_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Print final metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess a single image\n",
        "def preprocess_image(img_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(\n",
        "        img_path,\n",
        "        target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        "    )\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalize\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    return img_array\n",
        "\n",
        "# Function to make prediction\n",
        "def predict_image(img_path):\n",
        "    # Preprocess image\n",
        "    img_array = preprocess_image(img_path)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    # Convert prediction to class label\n",
        "    class_label = 'Dog' if prediction[0] > 0.5 else 'Cat'\n",
        "    confidence = prediction[0] if prediction[0] > 0.5 else 1 - prediction[0]\n",
        "\n",
        "    return class_label, confidence\n",
        "\n",
        "# Let's test on a few images from the test set\n",
        "test_folder = os.path.join(BASE_PATH, 'test1/test1')\n",
        "test_images = os.listdir(test_folder)[:5]  # Get first 5 test images\n",
        "\n",
        "for image_name in test_images:\n",
        "    image_path = os.path.join(test_folder, image_name)\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_class, confidence = predict_image(image_path)\n",
        "\n",
        "    # Display image and prediction\n",
        "    img = plt.imread(image_path)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f'Prediction: {predicted_class} (Confidence: {confidence:.2%})')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cdbRqxf7OrQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/dogcat_model'\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# Save the training history as a numpy file\n",
        "import numpy as np\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/training_history.npy', history.history)\n",
        "print(\"Training history saved\")"
      ],
      "metadata": {
        "id": "Eo_BHDugOzuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision Final Project: Dogs vs Cats Classification\n",
        "## By [Your Name]\n",
        "\n",
        "### 1. Problem Understanding and Dataset Selection (10%)\n",
        "#### Problem Definition\n",
        "This project addresses the fundamental computer vision task of binary image classification, specifically distinguishing between images of dogs and cats. This problem is significant because:\n",
        "- It demonstrates the capability of deep learning in recognizing complex visual patterns\n",
        "- It serves as a benchmark for testing various CNN architectures\n",
        "- It has practical applications in pet-related applications and animal monitoring systems\n",
        "\n",
        "#### Dataset Description\n",
        "- **Source**: Dogs vs Cats dataset from Kaggle\n",
        "- **Size and Distribution**:\n",
        "  - Total Images: 45,000\n",
        "  - Training Set: 25,000 (12,500 dogs, 12,500 cats)\n",
        "  - Validation Set: 8,000 (4,000 dogs, 4,000 cats)\n",
        "  - Test Set: 12,500 images\n",
        "- **Image Properties**: Color images of varying sizes and aspects ratios\n",
        "- **Dataset Quality**: High-quality images with clear subjects and diverse backgrounds\n",
        "\n",
        "### 2. Data Preprocessing (20%)\n",
        "#### Image Preprocessing\n",
        "1. **Resizing**:\n",
        "   - Standardized all images to 224x224 pixels\n",
        "   - Chosen to match VGG16's input requirements\n",
        "   - Preserves important features while managing computational resources\n",
        "\n",
        "2. **Normalization**:\n",
        "   - Scaled pixel values to range [0,1]\n",
        "   - Improves training stability and convergence\n",
        "\n",
        "#### Data Augmentation\n",
        "Implemented following transformations to prevent overfitting:\n",
        "- Rotation (±20 degrees)\n",
        "- Width/height shifts (20%)\n",
        "- Shear transformation (20%)\n",
        "- Zoom range (20%)\n",
        "- Horizontal flips\n",
        "- Fill mode: 'nearest'\n",
        "\n",
        "#### Data Organization\n",
        "- Structured directory hierarchy for train/validation/test sets\n",
        "- Implemented data generators for efficient memory usage\n",
        "- Maintained balanced class distribution\n",
        "\n",
        "### 3. Model Selection and Implementation (25%)\n",
        "#### Architecture Choice\n",
        "Selected VGG16 as the base model for transfer learning because:\n",
        "- Proven architecture for image classification\n",
        "- Pre-trained on ImageNet dataset\n",
        "- Good balance between complexity and performance\n",
        "\n",
        "#### Model Architecture\n",
        "1. **Base Model**:\n",
        "   - VGG16 (excluding top layers)\n",
        "   - Frozen weights to preserve learned features\n",
        "\n",
        "2. **Custom Layers**:\n",
        "   - Flatten layer to convert features to 1D\n",
        "   - Dense layer (512 units, ReLU activation)\n",
        "   - Dropout (0.5) for regularization\n",
        "   - Output layer (1 unit, sigmoid activation)\n",
        "\n",
        "3. **Parameters**:\n",
        "   - Total Parameters: 27,560,769\n",
        "   - Trainable Parameters: 12,846,081\n",
        "   - Non-trainable Parameters: 14,714,688\n",
        "\n",
        "### 4. Model Training and Evaluation (30%)\n",
        "#### Training Configuration\n",
        "- **Optimizer**: Adam\n",
        "- **Loss Function**: Binary Cross-entropy\n",
        "- **Metrics**: Accuracy\n",
        "- **Batch Size**: 32\n",
        "- **Epochs**: 15 with early stopping\n",
        "- **Early Stopping**: Patience=3, monitoring validation loss\n",
        "\n",
        "#### Training Results\n",
        "[Add your actual training results here:\n",
        "- Final training accuracy\n",
        "- Final validation accuracy\n",
        "- Training/validation loss curves]\n",
        "\n",
        "#### Evaluation Metrics\n",
        "[Add your model's performance metrics here:\n",
        "- Accuracy\n",
        "- Sample predictions on test images]\n",
        "\n",
        "### 5. Result Analysis and Discussion (15%)\n",
        "#### Model Performance\n",
        "[Discuss your model's performance, including:\n",
        "- Analysis of accuracy and loss curves\n",
        "- Discussion of any overfitting/underfitting\n",
        "- Analysis of misclassified images]\n",
        "\n",
        "#### Limitations\n",
        "1. Limited to binary classification\n",
        "2. Dependent on image quality\n",
        "3. Computational requirements of VGG16\n",
        "\n",
        "#### Future Improvements\n",
        "1. Experiment with other architectures (ResNet, EfficientNet)\n",
        "2. Implement cross-validation\n",
        "3. Try different learning rates and optimizers\n",
        "4. Add more dense layers\n",
        "5. Explore ensemble methods\n",
        "\n",
        "#### Conclusion\n",
        "[Add your concluding remarks about the project's success and learning outcomes]"
      ],
      "metadata": {
        "id": "419OgR8oPJiB"
      }
    }
  ]
}